{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/khhaledahmaad/sql-to-pandas-v1?scriptVersionId=143573896\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"__Unlocking the Power of SQL Analytics Functions in Pandas: A Data Science Journey!__","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://www.scaler.com/topics/images/sql-vs-pandas-thumbnail.webp\" width=500 alt=\"sql vs pandas\" />\n</center>\n\n<div style=\"text-align: center;\">\n    <txt>Image Source:<txt/>\n    <a href=\"https://www.scaler.com/topics/pandas/sql-vs-pandas/\">SCALER Topics</a>\n<div/>","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:45:58.66521Z","iopub.execute_input":"2023-09-18T20:45:58.66566Z","iopub.status.idle":"2023-09-18T20:45:58.673277Z","shell.execute_reply.started":"2023-09-18T20:45:58.665628Z","shell.execute_reply":"2023-09-18T20:45:58.671713Z"}}},{"cell_type":"markdown","source":"### Introduction\nIn the world of data science, the ability to seamlessly integrate powerful `SQL` analytics functions with the flexibility and versatility of `Pandas` in Python can be a game-changer. In this notebook, we'll embark on a journey to demonstrate how the capabilites of some of the formidable `SQL` analytics tools can be harmoniously transformed to supercharge your data manipulation and analysis capabilities in `Pandas`.","metadata":{}},{"cell_type":"markdown","source":"### Objective\nThe key objective of this demo is to demonstrate how efficient `Pandas` logics/fucntions can be formulated to achieve results like `SQL` style quick queries and results for the following:\n1. Joining and Mergeing datasets\n2. Advanced Analytics functions\n3. Nested and Repeated data\n\n__Note:__ \n1. In terms of querying large datasets in the cloud, it is much more efficient to use `SQL` queries for all sorts of merging and aggregations before transforming the datasets into Pandas dataframes. The main purpose of this notebook is to envision hypothetical scenarios where we need to perform similar SQL-like operations using Pandas when dealing with separate datasets from other sources, such as `CSV` files. `Pandas` does not currently have any built-in functionalities that allow us to use `SQL` queries directly. Additionally, when dealing with SQL-based databases or data warehouses, we can efficiently and quickly retrieve data, merge it as necessary, and even apply advanced aggregations, saving `CPU` usage and memory by utilising SQL at the query level.\n\n2. There are many ways and coding logics to achieve the results shown in this notebook. The examples provided are some of the easier ways to do so with the less possible codes.","metadata":{}},{"cell_type":"markdown","source":"### Prerequisite \n1. Basic understanding of `SQL`\n2. Basic understanding of `Pandas`\n3. Basic understainding of various `data structures` in programming\n4. Basic understanding of `SQL` style joins and their types (`INNER`, `OUTER`/`FULL`, `LEFT`, `RIGT`)\n5. Basic understanding of `databases`, `cloud`, and `API`","metadata":{}},{"cell_type":"markdown","source":"### Background\n1. When dealing with data, you may come across datasets that contain columns or attributes sharing identical values. In the context of Relational Database Management Systems, these are referred to as `foreign keys` [1]. For the sake of simplicity, we will simply call them `keys`.\" These keys exhibit identical values across various datasets, enabling the merging of different datasets into a single dataset based on the matching key values of each data point.\n\n2. Advanced analytics functions are frequently employed in SQL-based database queries. These functions enable the grouping of data and the application of sophisticated operations to various data points within the same groups. Here are some common analytics functions used in SQL [2].\n\n    2.1. __Analytic aggregate functions__\n\n    `MIN()` or `MAX()` - Returns the minimum (or maximum) of input values\n\n    `AVG()` or `SUM()` - Returns the average (or sum) of input values\n\n    `COUNT()` - Returns the number of rows in the input\n\n    2.2. __Analytic navigation functions__\n\n    `FIRST_VALUE()` or `LAST_VALUE()` - Returns the first (or last) value in the input\n\n    `LEAD()` and `LAG()` - Returns the value on a subsequent (or preceding) row\n\n    2.3. __Analytic numbering functions__\n\n    `ROW_NUMBER()` - Returns the order in which rows appear in the input (starting with 1)\n\n    `RANK()` - All rows with the same value in the ordering column receive the same rank value, where the next row receives a rank value which increments by the number of rows with the previous rank value.\n    \n3. In the most basic terms, `Nested` data refers to data structures resembling `JSON` objects or `Python` dictionaries that exist within the values of a column in a dataset. When there are multiple instances of these `Nested` objects enclosed within an array or list (denoted by `[]`), it is simply referred to as `Repeated` data. `Nested` data can be likened to a row of a table inside another table, whereas `Repeated` data represents multiple rows of a table nested inside another table. These concepts of `Nested` and `Repeated` data are commonly encountered in cloud-based databases or data retrieved from APIs, where the output data is often stored in `JSON` format.","metadata":{}},{"cell_type":"markdown","source":"### Setting the Stage: Importing Libraries and Data\nIn this demo, we've used the following:\n1. `Google Cloud Bigquery`, a cloud-based `Big Data` warehouse provided by `Google`.\n\n    __N.B.:__ In this notebook, we used `Kaggle's` public dataset BigQuery integration\n\n2. `SQL` to query the `BigQuery` public datasets. \n3. `Pandas` for data manipulation in `Python`.\n4. `JSON` to normalise `JSON` like objects","metadata":{}},{"cell_type":"code","source":"# Install the required libraries using the following commands (optional)\n# !pip install google-cloud-bigquery\n# !pip install pandas","metadata":{"hide_input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-19T22:39:11.412958Z","iopub.execute_input":"2023-09-19T22:39:11.413537Z","iopub.status.idle":"2023-09-19T22:39:11.420353Z","shell.execute_reply.started":"2023-09-19T22:39:11.413489Z","shell.execute_reply":"2023-09-19T22:39:11.418876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the libararies\nimport pandas as pd\nimport json\nfrom google.cloud import bigquery","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:11.423145Z","iopub.execute_input":"2023-09-19T22:39:11.423625Z","iopub.status.idle":"2023-09-19T22:39:11.439445Z","shell.execute_reply.started":"2023-09-19T22:39:11.423589Z","shell.execute_reply":"2023-09-19T22:39:11.437277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bigquery client object\nclient = bigquery.Client()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:11.441725Z","iopub.execute_input":"2023-09-19T22:39:11.442118Z","iopub.status.idle":"2023-09-19T22:39:11.452329Z","shell.execute_reply.started":"2023-09-19T22:39:11.442086Z","shell.execute_reply":"2023-09-19T22:39:11.451531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can set the `job_config` in the `client` object when passing the query to restrict the memory usage by the query to `1GB`.","metadata":{}},{"cell_type":"code","source":"# Create a safe job_config\n# only run the query if it's less than 1 GB\nONE_GB = 1000*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:11.453972Z","iopub.execute_input":"2023-09-19T22:39:11.454983Z","iopub.status.idle":"2023-09-19T22:39:11.464665Z","shell.execute_reply.started":"2023-09-19T22:39:11.454949Z","shell.execute_reply":"2023-09-19T22:39:11.463396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Joining and mergeing datasets\nHere, we will merge different datasets using both `SQL` and `Pandas`.","metadata":{}},{"cell_type":"markdown","source":"#### 1.1. `SQL JOINs` vs `pandas.DataFrame.join` or `pandas.DataFrame.merge`\nLet's consider an example where we merge two public datasets available in BigQuery: the `bigquery-public-data.usa_names.usa_1910_2013` dataset, which contains information about names and their occurrences in the United States, and the `bigquery-public-data.usa_names.usa_1960_2017` dataset, which contains similar information but for a different time period. We will merge these datasets using the names as a common key.","metadata":{}},{"cell_type":"code","source":"# Construct a reference to the \"usa_names\" dataset\ndataset_ref = client.dataset(\"usa_names\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:11.468794Z","iopub.execute_input":"2023-09-19T22:39:11.470011Z","iopub.status.idle":"2023-09-19T22:39:11.871656Z","shell.execute_reply.started":"2023-09-19T22:39:11.469965Z","shell.execute_reply":"2023-09-19T22:39:11.870095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List all the tables in the 'usa_names' dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are two!)\nfor table in tables:  \n    print(table.table_id)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:11.873414Z","iopub.execute_input":"2023-09-19T22:39:11.873808Z","iopub.status.idle":"2023-09-19T22:39:12.136601Z","shell.execute_reply.started":"2023-09-19T22:39:11.873775Z","shell.execute_reply":"2023-09-19T22:39:12.134977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will look at the `usa_1910_2013` table.","metadata":{}},{"cell_type":"code","source":"# Construct a reference to the \"usa_1910_2013\" table\ntable_ref = dataset_ref.table(\"usa_1910_2013\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:12.138177Z","iopub.execute_input":"2023-09-19T22:39:12.138557Z","iopub.status.idle":"2023-09-19T22:39:12.797944Z","shell.execute_reply.started":"2023-09-19T22:39:12.138526Z","shell.execute_reply":"2023-09-19T22:39:12.796638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also look at the `usa_1910_current` table.","metadata":{}},{"cell_type":"code","source":"# Construct a reference to the \"usa_1910_current\" table\ntable_ref = dataset_ref.table(\"usa_1910_current\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:12.79969Z","iopub.execute_input":"2023-09-19T22:39:12.80007Z","iopub.status.idle":"2023-09-19T22:39:13.636336Z","shell.execute_reply.started":"2023-09-19T22:39:12.800039Z","shell.execute_reply":"2023-09-19T22:39:13.634605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will join both tables on `name` as the `key`.","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:36:41.966838Z","iopub.execute_input":"2023-09-15T13:36:41.967257Z","iopub.status.idle":"2023-09-15T13:36:41.974212Z","shell.execute_reply.started":"2023-09-15T13:36:41.967223Z","shell.execute_reply":"2023-09-15T13:36:41.972599Z"}}},{"cell_type":"markdown","source":"#### 1.1.1. `SQL JOINs`","metadata":{}},{"cell_type":"code","source":"# Define the query\njoin_query = \"\"\"\nWITH L AS\n(\n  SELECT name, SUM(number) AS total_number_1910_current\n  FROM `bigquery-public-data.usa_names.usa_1910_current`\n  GROUP BY name\n  LIMIT 100\n),\n\nR AS \n(\n  SELECT name, SUM(number) AS total_number_1910_2013\n  FROM `bigquery-public-data.usa_names.usa_1910_2013`\n  WHERE name IN (SELECT name FROM L)\n  GROUP BY name\n)\n\nSELECT R.name, R.total_number_1910_2013, L.total_number_1910_current\nFROM R\nFULL JOIN L\nON L.name = R.name\nORDER BY L.total_number_1910_current DESC\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\njoin_result = client.query(join_query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', join_result.shape)\n# Preview of the top 5 rows\njoin_result.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:13.638717Z","iopub.execute_input":"2023-09-19T22:39:13.6391Z","iopub.status.idle":"2023-09-19T22:39:15.117843Z","shell.execute_reply.started":"2023-09-19T22:39:13.63907Z","shell.execute_reply":"2023-09-19T22:39:15.116374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explained!\nThe above code uses `CTEs (Common Table Expressions)` and `Aliases` in the `SQL` query. For more details, you can visit: \n\nhttps://www.kaggle.com/code/dansbecker/as-with\n\nThe above code simply pulls 100 common names available in each dataset and sums the total occurences of these names in each dataset. `Mary` is the most common name in the `USA` from `1910` to present. At least, the data says so!","metadata":{}},{"cell_type":"markdown","source":"#### 1.1.2. `pandas.DataFrame.join` or `pandas.DataFrame.merge`\nWe can achieve the same results using `Pandas`. The key difference between `Pandas` `join` and `merge` functions is that the `join` function joins datasets on a shared key column, if specified, which is named the same in each dataset and a column in the calling dataset and an index in the joining dataset, containing the shared key values. By default, it joins two or more datasets on the common indices if the key is not specified. The `merge` function allows SQL-style joining, where the key columns in each dataset can be specified separately. For more details, you can visit the following: \n\n[pandas.DataFrame.join](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join)\n\n[pandas.DataFrame.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html#pandas-dataframe-merge)\n","metadata":{}},{"cell_type":"code","source":"# Define your first query to create dataframe for the first dataset\nquery_1 = \"\"\"\nSELECT name, SUM(number) AS total_number_1910_current\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name\nLIMIT 100\n\"\"\"\n\n# Execute the first query and store the results in a Pandas DataFrame\nquery_job_1 = client.query(query_1)\ndf_1910_current = query_job_1.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:15.119426Z","iopub.execute_input":"2023-09-19T22:39:15.119864Z","iopub.status.idle":"2023-09-19T22:39:16.420955Z","shell.execute_reply.started":"2023-09-19T22:39:15.119829Z","shell.execute_reply":"2023-09-19T22:39:16.419562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your second query to create dataframe for the second dataset\nquery_2 = \"\"\"\nSELECT name, SUM(number) AS total_number_1910_2013\nFROM `bigquery-public-data.usa_names.usa_1910_2013`\nWHERE name IN (SELECT name FROM `bigquery-public-data.usa_names.usa_1910_current` GROUP BY name LIMIT 100)\nGROUP BY name\n\"\"\"\n\n# Execute the second query and store the results in another Pandas DataFrame\nquery_job_2 = client.query(query_2)\ndf_1910_2013 = query_job_2.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:16.422863Z","iopub.execute_input":"2023-09-19T22:39:16.423339Z","iopub.status.idle":"2023-09-19T22:39:17.839178Z","shell.execute_reply.started":"2023-09-19T22:39:16.423296Z","shell.execute_reply":"2023-09-19T22:39:17.837854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Note:__ We pulled only `100` rows from each dataset to conserve both memory and time since we are uncertain about the dataset sizes. If the datasets are significantly larger than the `CPU` usage and memory allocated for this notebook, the query might fail.","metadata":{}},{"cell_type":"markdown","source":"##### Using the `join` function:","metadata":{}},{"cell_type":"code","source":"# Merge the two DataFrames on the 'name' column\ndf_join = df_1910_2013.join(df_1910_current.set_index('name'), \n                              on='name', \n                              how='inner').sort_values('total_number_1910_current', # sort by total_number_1910_current\n                                                       ascending=False, # in descending\n                                                       ignore_index=True) # ignore_index=True to reset index in the merged dataset\n\n# Display number of rows and columns\nprint('(rows, cols):', df_join.shape)\n# Display the top 5 rows of the merged DataFrame\ndf_join.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:17.846088Z","iopub.execute_input":"2023-09-19T22:39:17.848304Z","iopub.status.idle":"2023-09-19T22:39:17.87233Z","shell.execute_reply.started":"2023-09-19T22:39:17.84824Z","shell.execute_reply":"2023-09-19T22:39:17.870609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Using the `merge` function: ","metadata":{}},{"cell_type":"code","source":"# Merge the two DataFrames on the 'name' column\ndf_merge = df_1910_2013.merge(df_1910_current,                                     # sort by total_number_1910_current\n                              on='name').sort_values('total_number_1910_current', # by default how='inner' when using merge \n                                                       ascending=False, \n                                                       ignore_index=True) # ignore_index=True to reset index in the merged dataset\n\n# Display number of rows and columns\nprint('(rows, cols):', df_merge.shape)\n# Display the top 5 rows of the merged DataFrame\ndf_merge.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:17.874263Z","iopub.execute_input":"2023-09-19T22:39:17.875641Z","iopub.status.idle":"2023-09-19T22:39:17.896823Z","shell.execute_reply.started":"2023-09-19T22:39:17.875581Z","shell.execute_reply":"2023-09-19T22:39:17.895476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nWe acheived the same results as `1.1.1.` but this time, we did not merge the datasets directly in the sql query, rather we pulled each datasets separately, then merged using both the `join` and `merge` functions in`Pandas`.","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the three dataframes are the same\njoin_result.equals(df_join) and df_join.equals(df_merge)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:17.898583Z","iopub.execute_input":"2023-09-19T22:39:17.898908Z","iopub.status.idle":"2023-09-19T22:39:17.909659Z","shell.execute_reply.started":"2023-09-19T22:39:17.89888Z","shell.execute_reply":"2023-09-19T22:39:17.907988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2. `SQL UNIONs` vs `pandas.DataFrame.concat`\nIn the previous examples, we joined the datasets on a common `key`. This time, we will join them row-wise.","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.1. `SQL` UNIONs","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:19:37.789249Z","iopub.execute_input":"2023-09-17T20:19:37.78972Z","iopub.status.idle":"2023-09-17T20:19:37.797857Z","shell.execute_reply.started":"2023-09-17T20:19:37.789682Z","shell.execute_reply":"2023-09-17T20:19:37.79612Z"}}},{"cell_type":"code","source":"# Define the query\nunion_query = \"\"\"\n WITH L AS \n (\n  SELECT state, gender, name, year, number\n  FROM `bigquery-public-data.usa_names.usa_1910_current`\n  LIMIT 100\n),\n\nR AS \n(\n  SELECT state, gender, name, year, number\n  FROM `bigquery-public-data.usa_names.usa_1910_2013`\n  WHERE name IN (SELECT name FROM L)\n)\n\nSELECT state, gender, name, year, number\nFROM L\nUNION ALL\nSELECT state, gender, name, year, number\nFROM R\nORDER BY state, gender, name, year, number\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\nunion_result = client.query(union_query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', union_result.shape)\n# Preview of the top 5 rows\nunion_result.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:17.912436Z","iopub.execute_input":"2023-09-19T22:39:17.91362Z","iopub.status.idle":"2023-09-19T22:39:29.807276Z","shell.execute_reply.started":"2023-09-19T22:39:17.913568Z","shell.execute_reply":"2023-09-19T22:39:29.806091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls `100` rows from the first dataset and the rows from the second dataset, where the names are common in the pulled `100` rows in the first dataset, and merges them into a single dataset.","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2. `pandas.DataFrame.concat`\n\nBy default, the `concat` function in Pandas merges rows of the datasets for common columns and appends any uncommon columns with their corresponding values to the corresponding rows. It fills in the missing values with `numpy.nan` at the corresponding index positions for columns with no values in the merged dataset. When the `axis` is set to `1`, it merges the columns of different datasets with values at their corresponding matching indices, leaving others as `numpy.nan`, similar to the join types described in section `1.1.`. If you set the `axis` to `1` and the `join` argument to `inner`, it will merge the row values of the columns with the same names in different datasets. Therefore, we can conclude that the `concat` function is the versatile data merging function in Pandas. For more infromation, you can visit the following:\n\n[pandas.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas-concat)","metadata":{}},{"cell_type":"code","source":"# Define your first query to create dataframe for the first dataset\nquery_1 = \"\"\"\nSELECT state, gender, name, year, number\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nLIMIT 100\n\"\"\"\n\n# Execute the first query and store the results in a Pandas DataFrame\nquery_job_1 = client.query(query_1)\ndf_1910_current = query_job_1.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:29.808793Z","iopub.execute_input":"2023-09-19T22:39:29.809142Z","iopub.status.idle":"2023-09-19T22:39:31.065695Z","shell.execute_reply.started":"2023-09-19T22:39:29.80911Z","shell.execute_reply":"2023-09-19T22:39:31.064345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your second query to create dataframe for the second dataset\nquery_2 = \"\"\"\nSELECT state, gender, name, year, number\nFROM `bigquery-public-data.usa_names.usa_1910_2013`\nWHERE name IN (SELECT name FROM `bigquery-public-data.usa_names.usa_1910_current` LIMIT 100)\n\"\"\"\n\n# Execute the second query and store the results in another Pandas DataFrame\nquery_job_2 = client.query(query_2)\ndf_1910_2013 = query_job_2.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:31.067499Z","iopub.execute_input":"2023-09-19T22:39:31.067844Z","iopub.status.idle":"2023-09-19T22:39:40.716391Z","shell.execute_reply.started":"2023-09-19T22:39:31.067813Z","shell.execute_reply":"2023-09-19T22:39:40.714972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the rows of the two DataFrames\ndf_concat = pd.concat([df_1910_current, df_1910_2013]).sort_values(\n                                                    ['state', 'gender', 'name', 'year', 'number'], ignore_index=True)\n                                                              # sort by state, gender, name, year, number\n                                                             # ignore_index=True to reset index in the merged dataset\n# Display number of rows and columns\nprint('(rows, cols):', df_concat.shape)\n# Display the top 5 rows of the merged DataFrame\ndf_concat.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:40.718583Z","iopub.execute_input":"2023-09-19T22:39:40.719621Z","iopub.status.idle":"2023-09-19T22:39:40.860924Z","shell.execute_reply.started":"2023-09-19T22:39:40.719574Z","shell.execute_reply":"2023-09-19T22:39:40.859397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls `100` rows from the first dataset and the rows from the second dataset, where the names are common in the pulled `100` rows in the first dataset into `Pandas` dataframes, and merge them into a single dataset.","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the two dataframes are the same\nunion_result.equals(df_concat)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:40.863686Z","iopub.execute_input":"2023-09-19T22:39:40.864181Z","iopub.status.idle":"2023-09-19T22:39:40.941976Z","shell.execute_reply.started":"2023-09-19T22:39:40.86413Z","shell.execute_reply":"2023-09-19T22:39:40.937939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Advanced Analytics Functions\nHere we will look at at least one example of different types of analytics functions can be defined in `SQL` and `Pandas`.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1. Analytic aggregate functions\nWe will create a cumulative sum of each name by `year` in the datasets, `usa_1910_current` (`1000` data points).","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1. `SQL` analytic aggregate functions\nWe will define analytic aggregate funtions using `SQL`.","metadata":{}},{"cell_type":"code","source":"# Define the query\nquery = \"\"\"\nWITH L AS\n(\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n)\n\nSELECT name, year, total_num,\n    SUM(total_num) \n        OVER (\n              PARTITION BY name\n              ORDER BY year\n              ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n             ) AS cum_sum\n    FROM L\n    ORDER BY name, year\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\ndf_query = client.query(query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', df_query.shape)\n# Preview of the top 5 rows\ndf_query.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:40.944517Z","iopub.execute_input":"2023-09-19T22:39:40.944963Z","iopub.status.idle":"2023-09-19T22:39:42.32417Z","shell.execute_reply.started":"2023-09-19T22:39:40.944927Z","shell.execute_reply":"2023-09-19T22:39:42.323006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls `1000` rows from the `usa_1910_current` dataset grouped by `name` and `year` and calculate a cummulative sum (cummulative total occurences) for each name from year to year ascending.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.2. Using `Pandas` for analytic aggregation\nWe will perform the above in a `Pandas` dataframe.\n\nWe will utilise the `pandas.DataFrame.groupby` function to accomplish this task. This function enables us to group data by one or more column values and apply various built-in aggregation functions provided by Pandas. Additionally, it allows us to apply custom functions using Pandas `lambda` expressions or user-defined functions. For more information, you can visit the following:\n\n[pandas.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)\n\n[pandas.core.groupby.DataFrameGroupBy.apply](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html#pandas.core.groupby.DataFrameGroupBy.apply)","metadata":{}},{"cell_type":"code","source":"# Define your query to create dataframe\nquery = \"\"\"\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n\"\"\"\n\n# Execute the query and store the results in Pandas DataFrame\nquery_job = client.query(query)\ndf_1910_current = query_job.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:42.326309Z","iopub.execute_input":"2023-09-19T22:39:42.327578Z","iopub.status.idle":"2023-09-19T22:39:43.777311Z","shell.execute_reply.started":"2023-09-19T22:39:42.327538Z","shell.execute_reply":"2023-09-19T22:39:43.775778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate a cummulative sum for each name year to year\n# Group by 'name', calculate cumulative sum, and reset index\ndf_grouped = (\n    df_1910_current\n    .groupby('name', as_index=False)\n    .apply(lambda df: df.assign(cum_sum=df.total_num.cumsum()))\n    .reset_index(drop=True)\n)\n\n# Display number of rows and columns\nprint('(rows, cols):', df_query.shape)\n# Preview of the top 5 rows\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:43.779016Z","iopub.execute_input":"2023-09-19T22:39:43.77973Z","iopub.status.idle":"2023-09-19T22:39:43.886338Z","shell.execute_reply.started":"2023-09-19T22:39:43.779681Z","shell.execute_reply":"2023-09-19T22:39:43.885181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls `1000` rows from the `usa_1910_current` dataset grouped by `name` and `year` and calculate a cummulative sum (cummulative total occurences) for each name from year to year ascending using `Pandas` `groupby` and `DataFrameGroupBy.apply` function.","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the two dataframes are the same\ndf_query.equals(df_grouped)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:43.887871Z","iopub.execute_input":"2023-09-19T22:39:43.888709Z","iopub.status.idle":"2023-09-19T22:39:43.897089Z","shell.execute_reply.started":"2023-09-19T22:39:43.888651Z","shell.execute_reply":"2023-09-19T22:39:43.895832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2. Analytic navigation functions\nHere, we will find the least and most common names in the dataset, `usa_1910_current` (`1000` data points).","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.1. `SQL` analytic navigation functions\nWe will define analytic navigation funtions using `SQL`.","metadata":{}},{"cell_type":"code","source":"# Define the query\nquery = \"\"\"\nWITH L AS\n(\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n)\n\nSELECT DISTINCT year,\n    FIRST_VALUE(name) \n        OVER (\n              PARTITION BY year\n              ORDER BY total_num\n              ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n             ) AS least_common_name,\n    LAST_VALUE(name) \n        OVER (\n              PARTITION BY year\n              ORDER BY total_num\n              ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n             ) AS most_common_name\n    FROM L\n    ORDER BY year\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\ndf_query = client.query(query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', df_query.shape)\nprint('The most common name(s) in this dataset:', df_query.most_common_name.unique())\n# Preview of the top 5 rows\ndf_query.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:43.89884Z","iopub.execute_input":"2023-09-19T22:39:43.899181Z","iopub.status.idle":"2023-09-19T22:39:45.140121Z","shell.execute_reply.started":"2023-09-19T22:39:43.899152Z","shell.execute_reply":"2023-09-19T22:39:45.138958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls the least and most common names in the `usa` by `year` from`1000` rows from the `usa_1910_current` dataset grouped by `name` and `year`.\n`Mary` is the most common name in the `usa` for the years present in this dataset!","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.2. Using `Pandas` for analytic navigation\nWe will perform the above in a `Pandas` dataframe.\n\nWe will utilise the `pandas.DataFrame.groupby` function to accomplish this task. This function enables us to group data by one or more column values and apply various built-in aggregation functions provided by Pandas. Additionally, it allows us to apply custom functions using Pandas `lambda` expressions or user-defined functions. For more information, you can visit the following:\n\n[pandas.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)\n\n[pandas.core.groupby.DataFrameGroupBy.apply](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html#pandas.core.groupby.DataFrameGroupBy.apply)","metadata":{}},{"cell_type":"code","source":"# Define your query to create dataframe\nquery = \"\"\"\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n\"\"\"\n\n# Execute the query and store the results in Pandas DataFrame\nquery_job = client.query(query)\ndf_1910_current = query_job.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:45.14167Z","iopub.execute_input":"2023-09-19T22:39:45.142Z","iopub.status.idle":"2023-09-19T22:39:46.657102Z","shell.execute_reply.started":"2023-09-19T22:39:45.141969Z","shell.execute_reply":"2023-09-19T22:39:46.655854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the least and most common names for each year\n# Group by 'year', find the least and most common names, and reset index\ndf_grouped = df_1910_current.groupby('year', as_index=False).apply(\n                lambda df: pd.Series({\n                    'least_common_name': df.loc[df['total_num'].idxmin(), 'name'],\n                    'most_common_name': df.loc[df['total_num'].idxmax(), 'name']\n                })\n            ).reset_index(drop=True)\n\n# Display number of rows and columns\nprint('(rows, cols):', df_grouped.shape)\nprint('The most common name(s) in this dataset:', df_grouped.most_common_name.unique())\n# Preview of the top 5 rows\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:46.658897Z","iopub.execute_input":"2023-09-19T22:39:46.659629Z","iopub.status.idle":"2023-09-19T22:39:46.709076Z","shell.execute_reply.started":"2023-09-19T22:39:46.65958Z","shell.execute_reply":"2023-09-19T22:39:46.707825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply pulls the least and most common names in the `usa` by `year` from`1000` rows from the `usa_1910_current` dataset grouped by `name` and `year`using `Pandas`.\n`Mary` is the most common name in the `usa` for the years present in this dataset!","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the two dataframes are the same\ndf_query.equals(df_grouped)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:46.710612Z","iopub.execute_input":"2023-09-19T22:39:46.711059Z","iopub.status.idle":"2023-09-19T22:39:46.726799Z","shell.execute_reply.started":"2023-09-19T22:39:46.711025Z","shell.execute_reply":"2023-09-19T22:39:46.725041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3. Analytic numbering functions\nHere, we will find the top `3` names by `year` in the dataset, `usa_1910_current` (`1000` data points).","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.1. `SQL` analytic numbering functions\nWe will define analytic numbering funtions using `SQL`.","metadata":{}},{"cell_type":"code","source":"# Define the query\nquery = \"\"\"\nWITH L AS\n(\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n)\n\nSELECT year,name\nFROM\n(\nSELECT year,name,\n    RANK() \n     OVER (\n           PARTITION BY year \n           ORDER BY total_num DESC) AS name_rank\n    FROM L) AS ranked_names\n\nWHERE name_rank <= 3\nORDER BY year,name_rank\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\ndf_query = client.query(query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', df_query.shape)\n# Preview of the top 5 rows\ndf_query.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:46.728628Z","iopub.execute_input":"2023-09-19T22:39:46.729029Z","iopub.status.idle":"2023-09-19T22:39:47.980738Z","shell.execute_reply.started":"2023-09-19T22:39:46.728998Z","shell.execute_reply":"2023-09-19T22:39:47.979524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply ranks the most common names in the `usa` by `year` from`1000` rows from the `usa_1910_current` dataset grouped by `name` and `year` and returns the top `3` names for each year.\nThe `RANK()` and `ROW_NUMBER()` have the same meanings, but the former one assigns the same rank for the same value of the ordering column, in this case `total_num`. For eaxample, if `2` lines have the same highest `total_num` they will be ranked `1`, and the next one in the partition will be ranked 2. But the `ROW_NUMBER()` functions rank in chronological order (1,2,3..etc.).","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.2. Using `Pandas` for analytic numbering\nWe will perform the above in a `Pandas` dataframe.\n\nWe will utilise the `pandas.DataFrame.groupby` function to accomplish this task. This function enables us to group data by one or more column values and apply various built-in aggregation functions provided by Pandas. Additionally, it allows us to apply custom functions using Pandas `lambda` expressions or user-defined functions. For more information, you can visit the following:\n\n[pandas.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)\n\n[pandas.core.groupby.DataFrameGroupBy.apply](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html#pandas.core.groupby.DataFrameGroupBy.apply)","metadata":{}},{"cell_type":"code","source":"# Define your query to create dataframe\nquery = \"\"\"\nSELECT name, year, SUM(number) AS total_num\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name, year\nLIMIT 1000\n\"\"\"\n\n# Execute the query and store the results in Pandas DataFrame\nquery_job = client.query(query)\ndf_1910_current = query_job.result().to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:47.982813Z","iopub.execute_input":"2023-09-19T22:39:47.984068Z","iopub.status.idle":"2023-09-19T22:39:49.313591Z","shell.execute_reply.started":"2023-09-19T22:39:47.984021Z","shell.execute_reply":"2023-09-19T22:39:49.312134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the top 3 names for each year\n# Group by 'name', find the top 3 names, and reset index\ndf_grouped = df_1910_current.groupby(['year']).apply(\n                    lambda df: df.sort_values('total_num', ascending=False)[['year', 'name']].head(3)\n).reset_index(drop=True)\n\n# Display number of rows and columns\nprint('(rows, cols):', df_grouped.shape)\n# Preview of the top 5 rows\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:49.315481Z","iopub.execute_input":"2023-09-19T22:39:49.315966Z","iopub.status.idle":"2023-09-19T22:39:49.389693Z","shell.execute_reply.started":"2023-09-19T22:39:49.315924Z","shell.execute_reply":"2023-09-19T22:39:49.388512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nThe above code simply ranks the most common names in the `usa` by `year` from`1000` rows from the `usa_1910_current` dataset grouped by `name` and `year` and returns the top `3` names for each year using `Pandas`.","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the two dataframes are the same\ndf_query.equals(df_grouped)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:49.391423Z","iopub.execute_input":"2023-09-19T22:39:49.391828Z","iopub.status.idle":"2023-09-19T22:39:49.402225Z","shell.execute_reply.started":"2023-09-19T22:39:49.391795Z","shell.execute_reply":"2023-09-19T22:39:49.400552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Nested and Repeated Data\nHere, we will transoform complex datasets into something like regular datasets. The complex datasets contain `Nested` and `Repeated` data, which are `JSON` or `Python` dictonary like objects in a list in the rows of a table as explained in the `Background`[3].\n\nFor this part of this notebook, we will use `BigQurey's` `github_repos` dataset, containining `GitHub` data.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1. Unnesting using `SQL`\nWe will define queries using `SQL` to unnest complex datasets.","metadata":{}},{"cell_type":"code","source":"# Construct a reference to the \"github_repos\" dataset\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:49.412218Z","iopub.execute_input":"2023-09-19T22:39:49.41265Z","iopub.status.idle":"2023-09-19T22:39:49.770208Z","shell.execute_reply.started":"2023-09-19T22:39:49.412617Z","shell.execute_reply":"2023-09-19T22:39:49.768918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List all the tables in the 'github_repos' dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are nine!)\nfor table in tables:  \n    print(table.table_id)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:49.771843Z","iopub.execute_input":"2023-09-19T22:39:49.772201Z","iopub.status.idle":"2023-09-19T22:39:50.052621Z","shell.execute_reply.started":"2023-09-19T22:39:49.77217Z","shell.execute_reply":"2023-09-19T22:39:50.051292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will look at the `languages` table.","metadata":{}},{"cell_type":"code","source":"# Construct a reference to the \"languages\" table\ntable_ref = dataset_ref.table(\"languages\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:50.053893Z","iopub.execute_input":"2023-09-19T22:39:50.054351Z","iopub.status.idle":"2023-09-19T22:39:50.881633Z","shell.execute_reply.started":"2023-09-19T22:39:50.054308Z","shell.execute_reply":"2023-09-19T22:39:50.879838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the query\nquery = \"\"\"\nSELECT \n    lang.name as name, lang.bytes as bytes\nFROM \n    `bigquery-public-data.github_repos.languages`,\nUNNEST(language) AS lang\nLIMIT 1000\n\"\"\"\n\n# Run the query, and return a pandas DataFrame\ndf_query = client.query(query).result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', df_query.shape)\n# Preview of the top 5 rows\ndf_query.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:50.883572Z","iopub.execute_input":"2023-09-19T22:39:50.884651Z","iopub.status.idle":"2023-09-19T22:39:52.499701Z","shell.execute_reply.started":"2023-09-19T22:39:50.884611Z","shell.execute_reply":"2023-09-19T22:39:52.498349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nWe simply unnested the objects from the `language` column and inserted them as separate rows(objects inside `[]`) and columns(objects inside `{}`) in the datasets.","metadata":{}},{"cell_type":"markdown","source":"#### 3.2. Using `Pandas` for analytic numbering\nWe will perform the above in a `Pandas` dataframe.\n\nTo achieve this we will define a function.","metadata":{}},{"cell_type":"code","source":"# Define your query to create dataframe for the nested dataset\nquery = \"\"\"\nSELECT \n    language\nFROM \n    `bigquery-public-data.github_repos.languages`\nLIMIT 1000\n\"\"\"\n\n# Execute the query and store the results in Pandas DataFrame\nquery_job = client.query(query)\ndf_nested = query_job.result().to_dataframe()\n# Display number of rows and columns\nprint('(rows, cols):', df_nested.shape)\n# Preview of the top 5 rows\ndf_nested.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:52.501492Z","iopub.execute_input":"2023-09-19T22:39:52.501946Z","iopub.status.idle":"2023-09-19T22:39:54.688092Z","shell.execute_reply.started":"2023-09-19T22:39:52.501893Z","shell.execute_reply":"2023-09-19T22:39:54.686488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to apply the trasformation\ndef unnest_data(df_nested, column_to_unnest): # pass the dataframe and the column from which unnested\n    \n    #  Other columns with indices\n    other_cols = df_nested.drop(columns=f'{column_to_unnest}')\n\n    # Separated rows with the duplicate indices for objects {} that was in the same []\n    nested_cols = df_nested.explode(column=f'{column_to_unnest}')\n    \n    # {} objects are transformed to dataframe\n    normalised_cols = pd.json_normalize(nested_cols[f'{column_to_unnest}'])\n\n    \"\"\"\n    The remaining columns with indices are joined along axis 1 (columns) with the normalized DataFrame. \n    In this process, additional rows may be generated. This is because, when the objects within {} are \n    extracted from [], the parent indices are duplicated. Consequently, the values from the other columns \n    will also be duplicated for these normalized rows or column values.\n    \"\"\"\n    df_unnested = pd.concat([other_cols, normalised_cols], axis=1).reset_index(drop=True)\n    \n    return df_unnested","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:54.690153Z","iopub.execute_input":"2023-09-19T22:39:54.690621Z","iopub.status.idle":"2023-09-19T22:39:54.70151Z","shell.execute_reply.started":"2023-09-19T22:39:54.690578Z","shell.execute_reply":"2023-09-19T22:39:54.699711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function and take 1000 rows\ndf_unnested = unnest_data(df_nested, 'language').iloc[:1000]\n# Display number of rows and columns\nprint('(rows, cols):', df_unnested.shape)\n# Preview of the top 5 rows\ndf_unnested.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:54.703771Z","iopub.execute_input":"2023-09-19T22:39:54.70469Z","iopub.status.idle":"2023-09-19T22:39:54.762329Z","shell.execute_reply.started":"2023-09-19T22:39:54.704642Z","shell.execute_reply":"2023-09-19T22:39:54.760876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nNot as simple as the `SQL` query, right!\n\nWe can also directly apply the transformation to the dataframe.","metadata":{}},{"cell_type":"code","source":"# Directly trasform the dataframe\n#  Other columns with indices\nother_cols = df_nested.drop(columns='language')\n\n# Separated rows with the duplicate indices for objects {} that was in the same []\nnested_cols = df_nested.explode(column=f'language')\n\n# {} objects are transformed to dataframe\nnormalised_cols = pd.json_normalize(nested_cols['language'])\n\n# Concatenate and return 1000 rows\ndf_unnested2 = pd.concat([other_cols, normalised_cols], axis=1).reset_index(drop=True).iloc[:1000]\n# Display number of rows and columns\nprint('(rows, cols):', df_unnested2.shape)\n# Preview of the top 5 rows\ndf_unnested2.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:54.76409Z","iopub.execute_input":"2023-09-19T22:39:54.764564Z","iopub.status.idle":"2023-09-19T22:39:54.820231Z","shell.execute_reply.started":"2023-09-19T22:39:54.764522Z","shell.execute_reply":"2023-09-19T22:39:54.818712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explained!\nWe simply unnested the objects from the `language` column and inserted them as separate rows(objects inside `[]`) and columns(objects inside `{}`) in the datasets using `Pandas`.","metadata":{}},{"cell_type":"markdown","source":"####  Let's Compare!\nThe dataframes we created so far should be all the same. Let's check, if this is true!","metadata":{}},{"cell_type":"code","source":"# Check if the two dataframes are the same\ndf_query.equals(df_unnested) and df_unnested.equals(df_unnested2)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T22:39:54.821877Z","iopub.execute_input":"2023-09-19T22:39:54.822383Z","iopub.status.idle":"2023-09-19T22:39:54.83196Z","shell.execute_reply.started":"2023-09-19T22:39:54.822351Z","shell.execute_reply":"2023-09-19T22:39:54.830264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nThis notebook demonstrates the interchangeability of `SQL` and `Pandas` functionalities. The inspiration behind this notebook stems from my past experiences with data manipulation using `Pandas`, particularly when I needed to apply `SQL` analytics to non-SQL datasets such as spreadsheets or CSV files.","metadata":{}},{"cell_type":"markdown","source":"### References\n[1]. Database.Guide(2016) _What is a Foreign Key?_ Available at: https://database.guide/what-is-a-foreign-key/, Accessed September 15, 2023.\n\n[2]. Cook, Alexis. \"Analytics Functions.\" Kaggle, Version 14.0, https://www.kaggle.com/code/alexisbcook/analytic-functions, Accessed September 15, 2023.\n\n[3]. Cook, Alexis. \"Nested and Repeated Data.\" Kaggle, Version 14.0, https://www.kaggle.com/code/alexisbcook/nested-and-repeated-data, Accessed September 15, 2023.","metadata":{}},{"cell_type":"markdown","source":"### Author\nKhaled Ahmed\n\n[LinkedIn](https://www.linkedin.com/in/ahmedkhaled40/)","metadata":{}}]}